---
format:
  pdf:
    pdf-engine: xelatex
    mainfont: "EB Garamond"
    fontsize: 10pt
    geometry:
      - "top = 1in"
      - "left = 1in"
      - "bottom = 1in"
      - "right = 0.5in"
    linestretch: 1
    indent: false
    number-sections: false
    link-citations: true
    header-includes:
      - \pagenumbering{gobble}
      - \usepackage[table]{xcolor}
      - \usepackage{setspace}
      - \frenchspacing
      - \renewcommand{\thefootnote}{\textbf{\Alph{footnote}}}

bibliography: references.bib
csl: https://www.zotero.org/styles/vancouver-superscript

editor: source
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| eval: true

# Load Packages and API Keys
source(paste(getwd(), "scripts/System_Config.R", sep = "/"))

# Import Raw Data
source(paste(getwd(), "scripts/Data_Import.R", sep = "/"))

# Final Data
source(paste(getwd(), "scripts/Data_Aggregate.R", sep = "/"))

# Analysis Data
source(paste(getwd(), "scripts/Method_Data.R", sep = "/"))
```

# Mapping the Shape of the U.S. Economy: A Topological Data Analysis Approach with BallMapper

*Ryan Johnson*\textsuperscript{*}

*[?] Department of Mathematics, University of Alaska Anchorage, Anchorage, AK*

Student: *johnson.ryan-0@pm.me*\textsuperscript{*} \newline
Mentor: *scook25@alaska.edu*

# KEYWORDS
Topological Data Analysis; Topology; TDA; BallMapper; Mapper; 
Data Science; Economics; Macroeconomics;

# ABSTRACT
Topological Data Analysis is (TDA) is a new data analysis method which gained popularity starting in the early 21st century. Currently, a large body of TDA research utilizes the traditional Mapper algorithm. We aim to fill an entry level gap into TDA as well as expand the body of literature using Ball Mapper--a Mapper adjacent algorithm. Using widely used U.S. macroeconomic data from the Bureau of Economic Analysis (BEA), the Federal Reserve Bank (FRB), and the Bureau of Labor Statistics (BLS), we achieve our goal by first showcasing Ball Mapper's use in Exploratory Data Analysis. We then combined Ball Mapper with the Fisher Equation, PPI-CPI Gap, and [ADD FINAL MEASURE] to show it's use beyond Exploratory Data Analysis. Our findings suggest that Ball Mapper is a useful data analysis tool to look at large multidimensional datasets and extract meaningful relationships in insights about our data quickly [...]

# INTRODUCTION
Topological Data Analysis (TDA) is a new and emerging field of data analysis that is increasing in popularity. Broadly, traditional TDA applications use two tools: an algorithm called Mapper that is represented by a mathematical graph and and analysis technique called Persistence Homology.[@madukpe2025mapper]\footnote{A mathematical graph, from Graph Theory, is most closely associated with a Network Graph that consits of nodes and edges.}
To provide a bridge from traditional statistical analysis, Mapper can be thought of the visual side and Persistent Homology as a the underlying theory and mathematics of our methodology.

In traditional statistical analysis, one might do some exploratory data analysis (EDA) looking at each variables distribution, variance, and  other summary statistics. That is, obtain information about the data to further make appropriate choices of which models to use for analysis.
Persistence Homology (PH) on the other hand is more akin to statistical models like the Linear Regression. To clarify this analogy, PH comes from mathematical fields of Algebraic Topology, and Linear Regression models have assumptions which use Probability Theory.[@kemme2025persistent; @trotta2023linear; @gfg_probability] Moreover, the combination of Mapper and Persistence Homology is what forms the central argument for TDA: data contains an underlying shape, and this shape can provide us with qualitative, and sometimes quantitative, insights about large multidimensional datasets.[@chazal2021introduction]

For this analysis we will be focusing on the graph creation portion of TDA. Specifically, we are examining various macroeconomic indicators of the United States of America (U.S.) using a Mapper-adjacent algorithm called BallMapper(BM).
Ball Mapper is of particular interest to us because it reduces the parameters needed for analysis. This is helpful because removes some of the barriers to start learning TDA by reducing the needed background and coding knowledge.
In traditional Mapper, one must pass the data through three stages, all with their own parameters, in order to generate a graph. Ball Mapper however only needs one parameter before producing a graph.[@dlotko2019ball] An important trade off between Ball Mapper and traditional Mapper is the following: BM reduces the number of steps needed to produce similar outcomes of traditional Mapper, but we lose control of being able fine tune out these outputs.
Those who have a mathematical background, introductory course in Topology, or conceptual knowledge of data science methods and algorithms will fare much easier to learning TDA. However, if you are not equipped with any of the aforementioned tools or knowledge, TDA might seem unnecessarily complicated for data analysis, or impossible to learn.

One major motivation for this paper is a paper written by  that applied TDABM to a global macroeconomic dataset to  We could not find other TDA literature found specifically focused on the macroeconomic economy of singular countries. Hence, our topic of choice.

One major motivation for this paper came from a paper by D\l{}otko, et al. [@dlotko2019mapping] This paper was a two pronged macroeconomic analysis using Ball Mapper. The first was a dataset of five macroeconomic variables from  16 countries from the 1870s-2017. It was interested in comparing how countries have evolved over time, their transformation from the Great Depression Era, and various views on wealth and inequality.
The second prong was to look at relationship between private credit growth and GDP of various countries. This paper was the only literature that we could find which combines both macroeconomics and Ball Mapper. Hence our choice of topic.

We hope that this paper will serve as an on-ramp for anyone interested in TDA but feels inundated with jargon upon early stage researching. If successful this analysis will also expand the small body of literature whose main focus is applications with Ball Mapper.[@madukpe2025mapper]

Using widely used U.S. macroeconomic data from the Bureau of Economic Analysis (BEA), the Federal Reserve Bank (FRB), and the Bureau of Labor Statistics (BLS), we plan to achieve these objectives by first showing Ball Mapper's use in exploratory data analysis (EDA). This includes general structural observations about our graphs as well as some more nuanced ways to interpret the Ball Mapper output. We follow this by extending Ball Mappers use in EDA to looking at its graphs through various economic lens: The Fisher Equation, [@mcclung2024fisher; @michaelides2024fisher] PPI\footnote{Producer Price Index - Finished Goods}-CPI Gap, [@bls_ppi_cpi_comparison] and [ADD LAST MEASURE].

# METHODS AND PROCEDURES

## *Data Selection & Preparation*
This paper relied on three publicly available data from US government sources. The Bureau of Economic Analysis (BEA), The Federal Reserve Board (FRB), and The Bureau of Labor Statistics (BLS).[@bea_data; @fred_data] The data were gathered using R using two application programming interfaces (APIs): one for data from the BEA, and the other from the Federal Reserve Economic Data (FRED) API.[@bea_r_package; @fredr_package] FRED aggregates data from national and international sources, as well as public and private sources. We additionally used recession dates based on business cycle contractions and expansions provided by the National Bureau of Economic Research (NBER). These agencies were selected because of they are authoritative sources for U.S. economic data. Their widespread use in both the private and public sector gives us high confidence in the accuracy and integrity of the data.[@hughes2019value]
```{r}
#| label: tbl-Data_Sources
#| tbl-cap: "*Housing Starts is considered a flow because it is provided as a Seasonally Adjusted Annual Rate (SAAR)."
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

source(paste(getwd(), "scripts/Table1_Data_Summary.R", sep = "/"))
data_summary_table
```
@tbl-Data_Sources shows all the data series we considered for this analysis. We excluded Employment Cost Index (ECI) and Foreign Direct Investment (FDI) due to their limited availability of years. Of the remaining data series, New Privately-Owned Housing Units Started (Housing Starts) has the smallest range and will provide the lower end for the years used in our analysis (1960-2024). @tbl-Analysis_Data shows the final data series we will be using. These specific series were chosen because they represent different aspects of the macroeconomy, and are described in *Functional Description*. Additionally, we are focusing on an annual time frame for this analysis so some data transformation was needed. The final data series transformations follow.
```{r}
#| label: tbl-Analysis_Data
#| tbl-cap: "All years for analysis are 1960-2024."
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Summary of Analysis Data
source(paste(getwd(), "scripts/Table2_Analysis_Data_Summary.R", sep = "/"))
analysis_data_summary_table
```

The first transformation was Personal Income and Its Disposition (Personal Income).\footnote{Table 2.1, BEA NIPA}
Personal Income is reported in nominal dollars, so it does not account for inflation. Thus, we first adjusted it using the Consumer Price Index (CPI) to get Personal Income into Real Dollars.[@dallasfed_nominal]
$$
\text{Real Dollars} = \frac{\text{Nominal Dollars}}{\text{CPI}} * 100
$$ {#eq-CPI_Real}
We then took the the log difference (@eq-deltaLog) from the output from @eq-CPI_Real to get a percentage change from the preceding year. This log difference helps to make our data more linear which aides BallMapper's use of the standard Euclidean Distance between each row of data. (CHECK - If BM did not use the Euclidean distance between points, we might not need to take log differences. However topic is for another paper.)[@hamilton2014logarithms]
$$
\Delta\ln{(\text{Level})} = 
[\ln{(\text{Level}_{t})} - \ln{(\text{Level}_{t-1})}]*100
$$ {#eq-deltaLog}

Our next transformation was on Housing Starts, Producer Price Index - Finished Goods (PPI) and CPI. We should note that PPI tracks only physical goods that businesses purchase either for their operations or to sell to consumers. Some examples are: fleet vehicles, appliances, groceries, gas, and clothing.[@bls_ppi_concepts]
All three of these sources were only provided on a monthly time frame, so to get an annual value we used a simple arithmetic mean.
Once these data are in annual form, we then found the percent change from the previous year using @eq-deltaLog for their final data in our analysis.

## METHODS

*BallMapper*
In depth descriptions of Ball Mapper's theory have been covered in various papers. [@dlotko2019ball; @rudkin2025ballmapper;@qiu2020refining] As the aim of our paper is to provided a introductory foray into BallMapper and TDA in general, we will omit some of the more technical details below but provide reference to more in-depth explanations.

Mathematically, the concept of Ball Mapper supposes we are given a dataset $X$ in $K$ dimensions with $N$ observations.
Then for some point $x \in X$ and given $\epsilon > 0$, we create a ball, $b(x, \epsilon)$, centered around $x$ with radius $\epsilon$.
Our aim to create set of balls, $B$, such that $B = \bigcup_{i = 0}^{n} b(x, \epsilon)$ for all $x \in B$.

We call our dataset $X$ a Point Cloud where each of the $N$ rows represents a point in our cloud. Our dimension, $K$, are each column of our data and commonly our point cloud has dimensions $K > 2$. When we are selecting a point, $x_i \in X$, to draw a ball, we are randomly selecting a row of our point cloud and then taking he Euclidean Distance in $K$ dimensions such that $i \neq j$ and $x_i, x_j \in X$ to every other $x_j$ in our point cloud. Then with our given epsilon, $\epsilon > 0$, we assign any points where the distance is less to epsilon to a ball with center $x_i$ (@eq-euclideanDistance).
$$
d(x_i, x_j) = 
\sqrt{(x_{i_1} - x_{j_1})^2 + (x_{i_2} - x_{j_2})^2 + \ldots + (x_{i_k} - x_{j_k})^2} < \epsilon
$$ {#eq-euclideanDistance}

As an algorithm, Ball Mapper is as follows:

1. Select a random point $x_i$ from point cloud $X$.
2. Given $\epsilon > 0$, construct a ball $b(x_i, \epsilon)$ with a center $x_i$ by associating all other points where $d(x_i, x_j) < \epsilon$ and $i \neq j$. (@eq-euclideanDistance)
3. Place $b_i, \epsilon)$ in a set of balls $B$.
4. Repeat steps 1-3 until all $x \in X$ belong to some $b(x, \epsilon)$.
5. Draw an edge between $b_i, b_j \in B$ if they contain the same $x$, weighting the edge based on number distinct $x$ values $b_i$ and $b_j$ both contain.

*Analysis*

As we saw above we need three parts for BallMapper: a Point Cloud, a variable to color by, and an epsilon. For this analysis, our point cloud was made up of 16 variables (dimensions) which can be see in @tbl-Analysis_Dimensions. All variables where transformed and two dimensions were created: *Unemployment Change* and *Fed Rate Change*. We elected to create these because we yield greater information seeing both the absolute number for each year and the direction they moved in over the course of a particular year; albeit, in a linear fashion. Put another way, unemployment is a flow, so it useful to see if the pressure in the hose of unemployment is building or dropping. Similarly, it is informative to see if changes in the Federal Funds Rate are having any effect on the economy.
```{r}
#| label: tbl-Analysis_Dimensions
#| tbl-cap: "Variables inculde in BallMapper Point Cloud"
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Summary of Analysis Data
source(paste(getwd(), "scripts/Table3_Analysis_Dimensions.R", sep = "/"))
analysis_dimensions_table
```
To our knowledge there is no generally accepted way of choosing epsilon in the literature. Since Ball Mapper constructs maps by selecting a point and creating a ball with a radius of epsilon, this is should in theory, given an epsilon small enough, produce a graph such that every point is a ball (node) of only itself. Conversely, this should lead to and epsilon big enough such that all of our points are included in one ball (node). For our analysis here, this is indeed the case. So to find an appropriate epsilon we first start by finding these lower and upper bounds.

What we look for in our lower bound is a graph with an epsilon small enough make every point in our point cloud a singleton node, not connected to any other.
For our upper bound we do the converse, we look for an epsilon so large where it creates a singular node that houses all of our points.
Additionally, to reduce some of the computational time, we look for a lower bound where a connected component started to form, and an upper bound where we had only two connected nodes.

For our data we first found a lower and upper bound which created an interval of $[0.38-0.90]$. We then applied a function to generated around 100 graphs and reviewed them, narrowing our interval to $[0.40-0.70]$. During this narrowing process we look for interesting features such as connected components forming or dissolving, flares coming off of any components, or notable sizing or coloration patterns.
We finally decided on the value $0.474$ because it presented a parts of all the aforementioned features we look for: coloration and size patterning, and connected components.

The standard Ball Mapper package in R produces a sufficient graph for exploratory data analysis. It's graphing function uses R base plot function and default coloring scheme. We however, felt that this graph output could be improved upon and recognized that we could use alternative, and more comprehensive packages to create our graph. That is, the output of the BallMapper algorithm could use an alternative package specifically for a network-like graph such as ours. Thus, translating the direct output from the BallMapper algorithm, we created the same graph but changed the visual representation and added another helpful piece of information that we were given but not used previously. Looking at @fig-newGraph, we have the standard Ball Mapper graph output on the left (@fig-newGraph-1) and our new graph output on the right (@fig-newGraph-2). The most prominent changes is the spread of the network and coloration.

```{r}
#| label: fig-newGraph
#| fig-cap: "Ball Mapper Graph colored by Year with Epsilon set to 0.511."
#| fig-subcap:
#|     - "Ball Mapper output using standard function (ColorIgraphPlot)"
#|     - "Ball Maper output using new graph output (ggraph)"
#| fig-cap-location: bottom
#| layout-ncol: 2
#| fig-width: 12
#| fig-height: 8
#| out-width: "100%"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Year) |>
    as.data.frame()

# Set Epsilon
e <- 0.474

# Create BM Output
BM_Compare <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Compare2 <- bm_to_igraph(BM_Compare)

# Output Maps
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))

# Asus: 654987, ThinkPad: 28716
ColorIgraphPlot(BM_Compare, seed_for_plotting = 654987)
bm_ggraph(BM_Compare2, coloring = coloring, epsilon = e)
```
Having the nodes spread out helps us see the structure of our graph more clearly as well as the new piece of information that was given to us from the Ball Mapper algorithm but not seen in @fig-newGraph-1, the edge strength. We see that in @fig-newGraph-2, highlighting the edge strength presents an insight about the relation between certain nodes. Although, our data consists of 65 rows, one could imagine the advantage this graph might have with a data set much larger that this one (e.g. looking at this data on a quarterly or monthly basis). Additionally, we changed the color palette for accessibility and readability. Looking at @fig-newGraph-1, any nodes colored in the royal blue (years approximately 2010-2015) mask the number identifier of the node.

For @fig-newGraph-2 and @fig-EDA, below, we use Year as our initial coloring variable. We started with this variable because D\l{}otko, et al. [@dlotko2019mapping] provided a framework of where to start with this type of analysis. This paper started by coloring their graphs by Year to initially understand the shape of their data. Later, in the second half of their analysis, they colored their graphs by each of the dimensions of a different pointcloud than the first. Because we have 16 dimensions to our pointcloud, a more in depth analysis is needed and is the the subject for another paper. Additionally, we should not that when D\l{}otko, et al. colored by Year, it was not in their pointcloud. Seeing that this could be useful, we extended this idea to our pointcloud and derived coloring variables that represented different economic lenses which we could view our graph. Namely, the Fisher Equation (@fig-Fisher), PPI-CPI Gap (@fig-Margin-Gap), and [ADD LAST MEASURE].

**General Interpretation*

Using @fig-EDA as our initial graph to explain, it is important to note with analysis such as this one, Ball Mapper will output the same graph unless at least one of two things changes: the size of the point cloud changes or the order of the point cloud. The former intuitively makes sense because the algorithm is creating epsilon radius balls and adding or subtracting any data will yield different results. Of course, depending on the data, the structure of the output might not change much. When choosing one variable, PPI, we had a multitude of choices and first tested PPI - All Commodities. We found a good epsilon at $0.511$ but for our analysis of PPI less CPI to understand the gap between the two, we though the PPI - Final Goods would be more insightful to look at. In fact there are 


# DISSCUSSION

**Exploratory Data Analysis (EDA) Application**

For our exploratory data analysis we colored our graph by the *Year*, @fig-EDA. The coloration of each node represents the average of all the years within in a node. The size of our node is commensurate with the number of points it contains. Between two nodes, th edge thickness is determined the number of same data points in each endpoint.Looking at @fig-EDA, we first observe the overall structure of our graph. There is one large connected component, one small component, and the rest singleton nodes.\footnote{A Connected Component is a subset of nodes in our graph such that each point in the subset has a path to all other points in the subset.} For the largest connected component (C1), we immediately are interested in the three large nodes (4, 24, and 26) because they are the largest in size for C1 and the same for their edge thickness (edge strength). Additionally, we also notice that each of the three large nodes have at least one flare (node connected only to themselves) as well as sharing others. What this structure tells us is, there are three distinct groups and each flare representing similar data to that specific group but perhaps is distinctly different to the other flares.
```{r}
#| label: fig-EDA
#| fig-cap: "Ball Mapper Graph colored by Year with Epsilon set to 0.474."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "100%"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Year) |>
    as.data.frame()

# Set Epsilon
e <- 0.474

# Create BM Output
BM_Year <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Year <- bm_to_igraph(BM_Year)

# Generate Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Year <- bm_ggraph(BM_Year, coloring = coloring, epsilon = e)

# investigate nodes
year_members <- map(.x = V(BM_Year),
                    .f = ~ {
                        final_data[V(BM_Year)$members[.x] |> unlist(), ]
                    })
# output map
ggraph_Year
```
The small component (C2), nodes 2 and 27, are of interest because it tells us us that these related nodes are somehow distinctly different from C1 and all other nodes. Seeing a small components like this sometimes can indicate outliers in data. In our case, this could be years where large economic events happened such as a recessions or extraordinary growth. However, we also can see this behavior in singleton nodes much like we have @fig-EDA. When we look at the coloration of the singleton nodes, we see indication of outlier events such as The Great Recession and the COVID-19 Pandemic.

To hark back to our discussion of C1, when we investigate nodes 4, 24, and 26 we indeed notice a pattern with the three generally: for the whole year the US was in an expansionary state. Node 4 shows us an "ideal" version of the economy. We have personal income and compensation slightly higher than inflation, sitting around 2%. Unemployment is a little elevated but historically not unusually high.
For Nodes 24 and 26 we see two different sides of large economic shocks. Node 24 shows us years where the economy is starting to get "over it's skis". That is, it is starting growing faster than it can keep up with but it's not necessarily too late. Node 26 on the other hand consists of years following right after recessions or points where the economy was over heating. Most of the years in Node 26, follow what economist call a "Jobless Recovery".[@kolesnikova2011jobless] Usually marked by growth returning to the economy but unemployment being "sticky" and not falling as business starts to take off.
For the flares and smaller points that are connect each of our large nodes in C1, we find that some years are in both the smaller node and larger one, which creates our edge. However they also contain additional years which are related nodes 4, 24, or 26 but instead describe transition years to these large nodes.

**Fisher Effect**

Using the Fisher Equation we can color our topological graph with the approximated real interested rate (@eq-fisherEquation).[@mcclung2024fisher] Although we know that the Fisher Equation is an approximation and does not hold well when nominal interest rates (Fed Rate) are relatively high and time spans are short, @fig-Fisher shows us a stark picture of our economic history and important time periods.[@michaelides2024fisher]

$$
\text{Real Interest Rate} \approx \text{Fed Rate} - \text{Inflation}
$$ {#eq-fisherEquation}

Looking at nodes 19-22, we notice that these years consists of the years 1981-1984, the years during and following the "Volker Shock".[@bryan2013greatinflation] What is interesting about why these nodes are colored is that these nodes see to show the impact of Volker's decision to raise interest and the lag that took place when raising rates.[@stlouisfed2023lags]

On the lower end the of Fisher Effect, we notice nodes 14, 28, 31, 33, 34 (1975, 2008, 2011-2012, 2021, 2022 respectively) show a negative value for @eq-fisherEquation. In these year rates were already extremely low, or the economy just came off a huge economic shock. In 1975 the fed aggressively lowered rates due to climbing inflation and high unemployment rates from the 1973-'74 oil embargo.[@corbett2013oilshock] In 2021-'22 the COVID-19 Pandemic stimulus checks were going to cause inflation but the fed did not want to raise rates too aggressively.

We also observe that for our C1 component, node 24 does show an approximate value from our Fisher Equation to be above the stated target rate of 2%. This indicates that indeed the economy for these nodes are showing signs of a "hot" economy.

```{r}
#| label: fig-Fisher
#| fig-cap: "Ball Mapper Graph colored by Fisher Equation with Epsilon set to 0.474."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "100%"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Fisher_Equation) |>
    as.data.frame()

# Set Epsilon
e <- 0.474

# Create BM Output
BM_Fisher <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Fisher <- bm_to_igraph(BM_Fisher)

# generate Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Fisher <- bm_ggraph(BM_Fisher, coloring = coloring, epsilon = e)

# investigate nodes
fisher_members <- map(.x = V(BM_Fisher),
                    .f = ~ {
                        final_data[V(BM_Fisher)$members[.x] |> unlist(), ]
                    })
# generate graph
ggraph_Fisher
```

**PPI-CPI Gap**

Measures the difference between rates of change between PPI - Finished Good and consumer inflation (CPI). Positive values indicated that businesses are having their revenue rise higher than consumer inflation. This could mean various things: corporate margins are expanding, they are passing through cost to customers, or....
```{r}
#| label: fig-Margin_Gap
#| fig-cap: "Ball Mapper Graph colored by the difference between CPI and PPI with Epsilon set to 0.474."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "100%"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Margin_Gap) |>
    as.data.frame()

# Set Epsilon
e <- 0.474

# Create BM Output
BM_Spread <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Spread <- bm_to_igraph(BM_Spread)

# Output Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Spread <- bm_ggraph(BM_Spread, coloring = coloring, epsilon = e)

# investigate nodes
spread_members <- map(.x = V(BM_Spread),
                    .f = ~ {
                        final_data[V(BM_Spread)$members[.x] |> unlist(), ]
                    })
# generate graph
ggraph_Spread
```


# CONCLUSION

# ACKNOWLEDGEMENTS

The author thanks Dr. Samuel Cook...

\newpage
# REFERENCES

:::{#refs}
:::


\newpage
# Old Paper


## Discussion

In our section we briefly went over two ways to interpret TDABM graphs. As seen in many other, longer papers written by Dlotko and colleagues, there is a lot of room to expound our TDABM graph here,though that is beyond the scope of this paper. In general, though, we can see that TDBM can give insight into our data that we might not see otherwise. Seeing that there is data that is not like the others gives an indication that there is something to investigate. Consider our comparison of the 08' - 09' financial crisis and the 2020 COVID Pandemic: We see learned that the years following the financial crisis were similar economically. Another way to view this is, clustering revels that the financial crisis was a singular problem that spurred the collapse of the financial system. Conversely, the COVID pandemic was a singular issue but exposed many different weaknesses in our current world.

For the future development of TDABM, and TDA in general, there are many possibilities of future research and one glaring downside. The downside to TDA is that it is a high barrier to entry to understand the methodologies inner workings. Those with a technical background will have an easier time, but nonetheless a higher barrier than methods such as linear regressions.  Subject wise, TDA has an advantage in some of the social sciences because it can be somewhat of a bridge between qualitative and quantitative analyses. TDABM has been used to analyze the Brexit Vote data, seeking to understand quantitative and qualitative motivations behind its outcome @Rudkin2023. TDA also has the advantages of dimension reduction which could be useful to the life-sciences. Fields such as genetics, chemistry, and biology have many fields of data, and being able to consolidate it into an understandable form could benefit the body of knowledge greatly.