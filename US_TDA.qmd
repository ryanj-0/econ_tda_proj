---
bibliography: references.bib

format:
  pdf:
    pdf-engine: xelatex
    mainfont: "EB Garamond"
    fontsize: 10pt
    geometry:
      - "top = 1in"
      - "left = 1in"
      - "bottom = 1in"
      - "right = 0.5in"
    linestretch: 1
    indent: false
    number-sections: false
    link-citations: true
    header-includes:
      - \pagenumbering{gobble}
      - \usepackage[table]{xcolor}



editor: source
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| eval: true

# Load Packages and API Keys
source(paste(getwd(), "scripts/System_Config.R", sep = "/"))

# Import Raw Data
source(paste(getwd(), "scripts/Data_Import.R", sep = "/"))

# Final Data
source(paste(getwd(), "scripts/Data_Aggregate.R", sep = "/"))

# Analysis Data
source(paste(getwd(), "scripts/Method_Data.R", sep = "/"))
```

# Mapping the Shape of the U.S. Economy: A Topological Data Analysis Approach with BallMapper

*Ryan Johnson*\textsuperscript{*}

*[?] Department of Mathematics, University of Alaska Anchorage, Anchorage, AK*

Student: *johnson.ryan1019@gmail.com*\textsuperscript{*} \newline
Mentor: *scook25@alaska.edu*

# KEYWORDS
Topological Data Analysis; BallMapper; Data Science;
Economics; Macroeconomics; Topology;

# ABSTRACT
Topological Data Analysis is (TDA) is a new data analysis method which gained popularity starting in the early 21st century. Currently, a large body of TDA research utilizes the traditional Mapper algorithm. We aim to fill an entry level gap into TDA as well as expand the body of literature on BallMapper, a new, Mapper adjacent algorithm. Using widely used U.S. macroeconomic data from the Bureau of Economic Analysis (BEA), the Federal Reserve Bank (FRB), and Bureau of Labor Statistics (BLS), we do this is three parts. 
We show an example of BallMapper's utility in exploratory data analysis which also provides an example of how to interpret BallMapper's output; analyze our topological graphs to notable historic economic events; and test the stability of BallMapper's output and the topological graph's features. Results show. [...]

# INTRODUCTION
Topological Data Analysis (TDA) is a new and emerging field of data analysis that is increasing in popularity. Broadly, traditional TDA applications use two tools: an algorithm called Mapper that produces a network-like graph and and analysis technique called Persistence Homology. 
Mapper can be thought of the exploratory data analysis (EDA) and visual display of statistical analysis. That is, we may see patterns in our data or need a visual tool to help explain the results of an analysis.
Persistence Homology on the other hand is more akin to statistical models like the Generalized Linear Model or Bayesian Statistics. They provide a mathematical framework and used as a tool to give a researcher confidence in where to look further or what they might be seeing in their data. Moreover, the combination of Mapper and Persistence Homology is what forms the central argument for TDA: data contains an underlying shape.\footnote{https://arxiv.org/pdf/2504.09042},\footnote{Chazal F and Michel B (2021) An Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists. Front. Artif. Intell. 4:667963. doi: 10.3389/frai.2021.667963}

For this analysis we will be focusing on the graph creation portion of TDA. Specifically, we are examining various macroeconomic indicators of the United States of America (U.S.) using a Mapper-adjacent algorithm called BallMapper(BM).\footnote{https://arxiv.org/pdf/1901.07410}
BallMapper is of particular interest to us because it significantly reduces the parameters to create a topological map. It simplifies traditional Mapper by reducing the need for the user to pass data through multiple functions, each with their own parameters.\footnote{add reference}
One benefit to this reduction in parameters is that it removes some of the barriers to learning Mapper (or Mapper-like algorithms), and more generally, TDA. Those who have a mathematical background, introductory course in Topology, or conceptual knowledge of data science methods and algorithms will fare much easier. However, if you are not equipped with any of the aforementioned tools, TDA might seem unnecessarily complicated for data analysis.

Beyond the main objectives of this analysis, we hope that this paper is serve as an on-ramp for anyone interested in TDA but feels inundated with jargon upon early stage researching. Out objectives are to add to the small body of literature whose main focus is applications with BallMapper.\footnote{add Mapper survey paper refrence here}
We go about this by first showing BallMapper's use in exploratory data analysis (EDA). Using our results from EDA, we then will compare BallMapper's outputs (topological graphs) to notable historic economic events. Lastly we will test the stability and consistency of BallMapper's output. The conclusion will talk broadly about results and other considerations of note, and the discussion will elaborate on future work and questions for research.


and is increasingly being applied to From the small body of literature we have reviewed, TDABM has removed many of the parameters that are required for more traditional methods of TDA. Generally speaking, traditional TDA contains four broad steps, each with multiple user-defined parameters. Conversely, TDABM has reduced this process to an the user selecting their data, a coloring variable and an epsilon value -- details on this follow below. What should be noted, although TDABM reduces the number of steps needed to produce similar outcomes of traditional TDA methods, we lose control of being able fine tune out outputs. We also find that interpreting results becomes more difficult. However, TDABM is still in its infancy, so there is not a large body of research on interpretation. [@dlotko2019ballmapper] One major motivation for this paper is a paper written by @dlotko2019macroeconomy that applied TDABM to a global macroeconomic dataset to compare how countries have evolved over time, their transformation from the Great Depression Era, and various views on wealth and inequality. We could not find other TDA literature found specifically focused on the macroeconomic economy of singular countries. Hence, our topic of choice.

# METHODS AND PROCEDURES

## *Data Selection & Preparation*
This paper relied on three publicly available data from US government sources. The Bureau of Economic Analysis (BEA), The Federal Reserve Board (FRB), and The Bureau of Labor Statistics (BLS). The data were gathered using R using two application programming interfaces (APIs): one for data from the BEA, and the other from the Federal Reserve Economic Data (FRED) API. FRED aggregates data from national and international sources, as well as public and private sources. We additionally used recession dates based on business cycle contractions and expansions provided by the National Bureau of Economic Research (NBER). These agencies were selected because of they are authoritative sources for U.S. economic data. Their widespread use in both the private and public sector gives us high confidence in the accuracy and integrity of the data.\footnote{Hughes-Cromwick, Ellen, and Julia Coronado. 2019. "The Value of US Government Data to US Business Decisions." Journal of Economic Perspectives 33 (1): 131\u201346. DOI: 10.1257/jep.33.1.131}
```{r}
#| label: tbl-Data_Sources
#| tbl-cap: "*Housing Starts is considered a flow because it is provided as a Seasonally Adjusted Annual Rate (SAAR)."
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

source(paste(getwd(), "scripts/Table1_Data_Summary.R", sep = "/"))
data_summary_table
```
@tbl-Data_Sources shows all the data series we considered for this analysis. We excluded Employment Cost Index (ECI) and Foreign Direct Investment (FDI) due to their limited availability of years. Of the remaining data series, New Privately-Owned Housing Units Started (Housing Starts) has the smallest range and will provide the lower end for the years used in our analysis (1960-2024). @tbl-Analysis_Data shows the final data series we will be using. These specific series were chosen because they represent different aspects of the macroeconomy, and are described in *Functional Description*. Additionally, we are focusing on an annual time frame for this analysis so some data transformation was needed. The final data series transformations follow.
```{r}
#| label: tbl-Analysis_Data
#| tbl-cap: "All years for analysis are 1960-2024."
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Summary of Analysis Data
source(paste(getwd(), "scripts/Table2_Analysis_Data_Summary.R", sep = "/"))
analysis_data_summary_table
```

The first transformation was Personal Income and Its Disposition (Personal Income).\footnote{Table 2.1 from the BEA's Interactive Data, National Data section.}
Personal Income is reported in nominal dollars, so it does not account for inflation. Thus, we first adjusted it using the Consumer Price Index (CPI) to get Personal Income into Real Dollars.\footnote{https://www.dallasfed.org/research/basics/nominal}
$$
\text{Real Dollars} = \frac{\text{Nominal Dollars}}{\text{CPI}} * 100
$${#eq-CPI_Real}
We then took the the log difference (@eq-deltaLog) from the output from @eq-CPI_Real to get a percentage change from the preceding year. This log difference helps to make our data more linear which aides BallMapper's use of the standard Euclidean Distance between each row of data. [If BM did not use the Euclidian distance between points, we might not need to take log differences. However topic is for another paper.]
\footnote{https://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics}
\footnote{https://www.numberanalytics.com/blog/log-transform-econ-how-to-guide}
$$
\Delta\ln{(\text{Level})} = 
[\ln{(\text{Level}_{t})} - \ln{(\text{Level}_{t-1})}]*100
$${#eq-deltaLog}

Our next transformation was on Housing Starts, Producer Price Index - All Commodities (PPI)\footnote{PPI tracks only physical goods such as Farm Products and Feed, Industrial Commodidities, and Other Commodidites such as Aircraft, Steel, and Lumber.}, and CPI. All three of these sources were only provided on a monthly time frame, so to get an annual value we used a simple arithmetic mean.
Once these data are in annual form, we then found the percent change from the previous year using @eq-deltaLog for their final data in our analysis.

[Need to add parts more about the data itself?]

## METHODS

### *BallMapper*
In depth descriptions of Ball Mapper's theory have been covered in various papers. As the aim of our paper is to provided a introductory foray into BallMapper and TDA in general, we will omit some of the more technical details below but provide reference to more in-depth explanations.

Mathematically, the concept of Ball Mapper supposes we are given a dataset $X$ in $K$ dimensions with $N$ observations.
Then for some point $x \in X$ and given $\epsilon > 0$, we create a ball, $b(x, \epsilon)$, centered around $x$ with radius $\epsilon$.
Our aim to create set of balls, $B$, such that $B = \bigcup_{i = 0}^{n} b(x, \epsilon)$ for all $x \in B$.

We call our dataset $X$ a Point Cloud where each of the $N$ rows represents a point in our cloud. Our dimension, $K$, are each column of our data and commonly our point cloud has dimensions $K > 2$. When we are selecting a point, $x_i \in X$, to draw a ball, we are randomly selecting a row of our point cloud and then taking he Euclidean Distance in $K$ dimensions such that $i \neq j$ and $x_i, x_j \in X$ to every other $x_j$ in our point cloud. Then with our given epsilon, $\epsilon > 0$, we assign any points where the distance is less to epsilon to a ball with center $x_i$ (@eq-euclideanDistance).
$$
d(x_i, x_j) = 
\sqrt{(x_{i_1} - x_{j_1})^2 + (x_{i_2} - x_{j_2})^2 + \ldots + (x_{i_k} - x_{j_k})^2} < \epsilon
$${#eq-euclideanDistance}

Algorithmically, Ball Mapper is as follows:

1. Select a random point $x_i$ from point cloud $X$.
2. Given $\epsilon > 0$, construct a ball $b(x_i, \epsilon)$ with a center $x_i$ by associating all other points where $d(x_i, x_j) < \epsilon$ and $i \neq j$.\footnote{See @eq-euclideanDistance for $d(x_i, x_j)$}
3. Place $b_i, \epsilon)$ in a set of balls $B$.
4. Repeat steps 1-3 until all $x \in X$ belong to some $b(x, \epsilon)$.
5. Draw an edge between $b_i, b_j \in B$ if they contain the same $x$, weighting the edge based on number distinct $x$ values $b_i$ and $b_j$ both contain.

### *Analysis*
As we saw above we need three parts for BallMapper: a Point Cloud, a variable to color by, and an epsilon. For this analysis, our point cloud was made up of 16 variables (dimensions) which can be see in @tbl-Analysis_Dimensions. It is important to note two dimensions in our analysis were created: *Unemployment Change* and *Fed Rate Change*. [Add reasoning] All other variables where transformed but no new data created from them. Albeit, Unemployment is a lagging indicator and the Federal Funds Rate is a policy or reactive choice, we thought these two measures were important. Since Unemployment is a flow, is it useful to see if the hose of unemployed is expanding or shrinking. Similarly, it is informative to see changes in the Federal Funds Rate to see the policy choices made are working or not.
```{r}
#| label: tbl-Analysis_Dimensions
#| tbl-cap: "Variables inculde in BallMapper Point Cloud"
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Summary of Analysis Data
source(paste(getwd(), "scripts/Table3_Analysis_Dimensions.R", sep = "/"))
analysis_dimensions_table
```
To our knowledge there is no generally accepted way of choosing epsilon in the literature. Since BallMapper constructs maps by selecting a point and taking a radius distance of epsilon, this is should in theory, if given an epsilon small enough, produce a graph such that every point is its own ball. Conversely, this should lead to all of our points being included in one node. For our analysis here, this is indeed the case. So to find an appropriate epsilon we first start by finding these lower and upper bounds.
What we look for in our lower bound is a graph is an epsilon small enough make every point in our point cloud a singleton node, not connected to any other.
For our upper bound we do the converse and look for an epsilon so large where it creates a singular node that houses all of our points.
Additionally, to reduce some of the computational time, we look for a lower bound where a connected component started to form, and an upper bound where we had only two connected nodes.

For our data we first found a lower and upper bound which created an interval of $[0.3-1.4]$. We then applied a function to generated around 100 graphs and reviewed them, narrowing our interval to $[0.4-0.75]$. During this narrowing process we look for interesting features such as connected components forming or dissolving, flares coming off of any components, or notable sizing or coloration patterns.
We finally decided on the value $0.511$ because it presented a little of all the aforementioned features we look for: coloration and size patterning and connected components.

The standard BallMapper package in R produces a sufficient graph for exploratory data analysis. It's graphing function uses R base plot function and default coloring scheme. We however, felt that this graph output could be improved upon and recognized that we could use alternative, and more comprehensive packages to create our graph. That is, the output of the BallMapper algorithm could use an alternative package specifically for a network-like graph such as ours. Thus, translating the direct output from the BallMapper algorithm, we created the same graph but changed the visual representation and added a few more help bits of information that we were given but not used previously. Looking at @fig-newGraph, we have the standard Ball Mapper graph output on the left (@fig-newGraph-1) and our new graph output on the right (@fig-newGraph-2). The most prominent changes is the spread of the network and coloration. 

```{r}
#| label: fig-newGraph
#| fig-cap: "Ball Mapper Graph colored by Year with Epsilon set to 0.511."
#| fig-subcap:
#|     - "Ball Mapper output using standard function (ColorIgraphPlot)"
#|     - "Ball Maper output using new graph output (ggraph)"
#| fig-cap-location: bottom
#| layout-ncol: 2
#| fig-width: 12
#| fig-height: 8
#| out-width: "80%"
#| fig-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Year) |>
    as.data.frame()

# Set Epsilon
e <- 0.511

# Create BM Output
BM_Compare <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Compare2 <- bm_to_igraph(BM_Compare)

# Output Maps
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))

ColorIgraphPlot(BM_Compare, seed_for_plotting = 28716)
bm_ggraph(BM_Compare2, coloring = coloring, epsilon = e)
```

For our initial map we used *Year* as our coloring variable. This is because Year is a metric that is not dependent on any economic variables and is a good marker for large macroeconomic events such as the dot com bubble of the 2000s. *Year* is not included in the construction of our Point Cloud. This is an important point because we show later that being able to create metrics to color by adds a depth the Ball Mapper's usefulness, even if the metrics themselves are not included in our Point Cloud.

[While the standard implenmentation of BallMapper provides and output...be clear about ballmapper output being the same as ggraph output]
Instead of using the graph output provided by BallMapper, we created graph output using other R packages to bring clarity to our analysis. We have not changed the core structure of the BallMapper output, but rather the presentation. The largest changes of note to the graphs are: spread out nodes to avoid overlapping; display the edge weights between nodes, representing the number number of shared values between two nodes; custom color scale for accessibility.

# CONCLUSION
**Exploratory Data Analysis (EDA) Application**

For our exploratory data analysis exploration we colored our graph by the *Year*, @fig-EDA. The coloration of each node represents the average of all the years within in a node. The size of our node is commensurate with the number of years it houses. Between two nodes, th edge thickness is determined the number of same data points in each endpoint.Looking at @fig-EDA, we first observe the overall structure of our graph. There is one large connected component, one small component, and the rest singleton nodes.\footnote{A Connected Component is a subset of nodes in our graph such that each point in the subset has a path to all other points in the subset.} For the largest connected component (C1), we immediately are interested in the three large nodes (4, 24, and 26) because they are the largest in size for C1 and the same for their edge thickness (edge strength). Additionally, we also notice that each of the three large nodes have at least one flare (node connected only to themselves) as well as sharing others. What this structure tells us is, there are three distinct groups and each flare representing similar data to that specific group but perhaps is distinctly different to the other flares.
```{r}
#| label: fig-EDA
#| fig-cap: "Ball Mapper Graph colored by Year with Epsilon set to 0.511."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "80%"
#| fig-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Year) |>
    as.data.frame()

# Set Epsilon
e <- 0.511

# Create BM Output
BM_Year <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Year <- bm_to_igraph(BM_Year)

# Output Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Year <- bm_ggraph(BM_Year, coloring = coloring, epsilon = e)
ggraph_Year
```
The small component (C2), nodes 2 and 27, are of interest because it tells us us that these related nodes are somehow distinctly different from C1 and all other nodes. Seeing a small components like this sometimes can indicate outliers in data. In our case, this could be years where large economic events happened such as a recessions or extraordinary growth. However, we also can see this behavior in singleton nodes much like we have @fig-EDA. When we look at the coloration of the singleton nodes, we see indication of outlier events such as The Great Recession and the COVID-19 Pandemic.

To hark back to our discussion of C1, when we investigate nodes 4, 24, and 26 we indeed notice a pattern with the three generally: for the whole year the US was in an expansionary state. Node 4 shows us an "ideal" version of the economy. We have personal income and compensation slightly higher than inflation, sitting around 2%. Unemployment is a little elevated but historically not unusually high.
For Nodes 24 and 26 we see two different sides of large economic shocks. Node 24 shows us years where the economy is starting to get "over it's skis". That is, it is starting growing faster than it can keep up with but it's not necessarily too late. Node 26 on the other hand consists of years following right after recessions or points where the economy was over heating. Most of the years in Node 26, follow what economist call a "Jobless Recovery".\footnote{https://www.stlouisfed.org/publications/regional-economist/april-2011/jobless-recoveries-causes-and-consequences} Usually marked by growth returning to the economy but unemployment being "sticky" and not falling as business starts to take off.
For the flares and smaller points that are connect each of our large nodes in C1, we find that some years are in both the smaller node and larger one, which creates our edge. However they also contain additional years which are related nodes 4, 24, or 26 but instead describe transition years to these large nodes.

**Fisher Effect**
Using the Fisher Equation we can color our topological graph with the approximated real interested rate (@fisherEquation).\footnote{https://inomics.com/terms/fisher-effect-1545486}. Although we know that the Fisher Equation is an approximation and does not hold well when nominal interest rates (Fed Rate) are relatively high and time spans are short, @fig-Fisher shows us a stark picture of our economic history and important time periods.\footnote{https://link.springer.com/chapter/10.1007/978-3-031-76140-9\_9}

$$
\text{Real Interest Rate} \approx \text{Fed Rate} - \text{Inflation}
$${#eq-fisherEquation}

Looking at nodes 19-22, we notice that these years consists of the years 1981-1984, the years during and following the "Volker Shock".\footnote{https://www.federalreservehistory.org/essays/great-inflation} What is interesting about why these nodes are colored is that these nodes see to show the impact of Volker's decision to raise interest and the lag that took place when raising rates.\footnote{https://www.stlouisfed.org/on-the-economy/2023/oct/what-are-long-variable-lags-monetary-policy}

On the lower end the of Fisher Effect, we notice nodes 14, 28, 31, 33, 34 (1975, 2008, 2011-2012, 2021, 2022 respectively) show a negative value for @eq-fisherEquation. In these year rates were already extremely low, or the economy just came off a huge economic shock. In 1975 the fed aggressively lowered rates due to climbing inflation and high unemployment rates from the 1973-'74 oil embargo.\footnote{https://www.federalreservehistory.org/essays/oil-shock-of-1973-74} In 2021-'22 the COVID-19 Pandemic stimulus checks were going to cause inflation but the fed did not want to raise rates too aggressively.

We also observe that for our C1 component, node 24 does show an approximate value from our Fisher Equation to be above the stated target rate of 2%. This indicates that indeed the economy for these nodes are showing signs of a "hot" economy.

```{r}
#| label: fig-Fisher
#| fig-cap: "Ball Mapper Graph colored by Fisher Equation with Epsilon set to 0.511."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "80%"
#| fig-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(Fisher_Equation) |>
    as.data.frame()

# Set Epsilon
e <- 0.511

# Create BM Output
BM_Fisher <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Fisher <- bm_to_igraph(BM_Fisher)

# Output Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Fisher <- bm_ggraph(BM_Fisher, coloring = coloring, epsilon = e)
ggraph_Fisher
```

```{r}
#| label: fig-PPI_CPI_Spread
#| fig-cap: "Ball Mapper Graph colored by the difference between PPI and CPI with Epsilon set to 0.511."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 8
#| out-width: "80%"
#| fig-pos: "H"
#| echo: false
#| eval: true
#| warning: false

# Set Year Coloring
coloring <- final_data |>
    select(PPI_CPI_Spread) |>
    as.data.frame()

# Set Epsilon
e <- 0.511

# Create BM Output
BM_Spread <- BallMapper(points = pointcloud, values = coloring, epsilon = e)
BM_Spread <- bm_to_igraph(BM_Spread)

# Output Map
source(paste(getwd(), "functions/bm_ggraph.R", sep = "/"))
ggraph_Spread <- bm_ggraph(BM_Spread, coloring = coloring, epsilon = e)
ggraph_Spread
```

### *Robustness and Stabilization*
- Shuffle Dataset --> Run BallMapper
  - Look at:
    - # nodes
    - coloring consistency
    - look at financne application and brexit vote


# DISCUSSION
# REFERENCES


# SUPPLEMENTARY

```{r}
node_4 <- final_data[V(BM_Year)$members[4] |> unlist(), ]
node_24 <- final_data[V(BM_Year)$members[24] |> unlist(), ]
node_26 <- final_data[V(BM_Year)$members[26] |> unlist(), ]

node_4 <- node_4 |> 
  summarise(across(-Year, mean)) |> 
  pivot_longer(everything(), names_to = "Measure", 
               values_to = "Node 4 Avg. - The Sweet Spot")
node_24 <- node_24 |>
  summarise(across(-Year, mean)) |> 
  pivot_longer(everything(), names_to = "Measure", 
               values_to = "Node 24 Avg. - Overheating")
node_26 <- node_26 |> 
  summarise(across(-Year, mean)) |> 
  pivot_longer(everything(), names_to = "Measure", 
               values_to = "Node 26 Avg. - Healthy Recovery")

all_nodes <- node_4 |> 
  left_join(node_24, by = "Measure") |> 
  left_join(node_26, by = "Measure") |>
  filter(Measure != "row_id") |>
  gt() |>
  opt_row_striping()
```

```{r}
#| label: tbl-C1_4
#| tbl-cap: "Component C1, Node 4 Members"
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

final_data[V(BM_Year)$members[4] |> unlist(), ] |>
  gt() |>
  tab_options(table.width = pct(100))
```

```{r}
#| label: tbl-C1_24
#| tbl-cap: "Component C1, Node 24 Members"
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

final_data[V(BM_Year)$members[24] |> unlist(), ] |>
  gt() |>
  tab_options(table.width = pct(100))
```

```{r}
#| label: tbl-C1_26
#| tbl-cap: "Component C1, Node 26 Members"
#| tbl-cap-location: bottom
#| tbl-pos: "H"
#| echo: false
#| eval: true
#| warning: false

final_data[V(BM_Year)$members[26] |> unlist(), ] |>
  gt() |>
  tab_options(table.width = pct(100))
```
# Old Paper


## Discussion

This paper serves as a proof-of-concept of the usefulness of TDABM as a methodology for exploratory data analysis. By no means is this a totally comprehensive method to gain insight into data, but instead TDA BallMapper proves itself to be useful tool wehen employed in addition to other traditional analyses. 

In our section we briefly went over two ways to interpret TDABM graphs. As seen in many other, longer papers written by Dlotko and colleagues, there is a lot of room to expound our TDABM graph here,though that is beyond the scope of this paper. In general, though, we can see that TDBM can give insight into our data that we might not see otherwise. Seeing that there is data that is not like the others gives an indication that there is something to investigate. Consider our comparison of the 08' - 09' financial crisis and the 2020 COVID Pandemic: We see learned that the years following the financial crisis were similar economically. Another way to view this is, clustering revels that the financial crisis was a singular problem that spurred the collapse of the financial system. Conversely, the COVID pandemic was a singular issue but exposed many different weaknesses in our current world.

For the future development of TDABM, and TDA in general, there are many possibilities of future research and one glaring downside. The downside to TDA is that it is a high barrier to entry to understand the methodologies inner workings. Those with a technical background will have an easier time, but nonetheless a higher barrier than methods such as linear regressions.  Subject wise, TDA has an advantage in some of the social sciences because it can be somewhat of a bridge between qualitative and quantitative analyses. TDABM has been used to analyze the Brexit Vote data, seeking to understand quantitative and qualitative motivations behind its outcome @Rudkin2023. TDA also has the advantages of dimension reduction which could be useful to the life-sciences. Fields such as genetics, chemistry, and biology have many fields of data, and being able to consolidate it into an understandable form could benefit the body of knowledge greatly.


## Data Source

- Bureau of Economic Analysis
    -   [Real Exports and Imports](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDNdLCJkYXRhIjpbWyJjYXRlZ29yaWVzIiwiU3VydmV5Il0sWyJOSVBBX1RhYmxlX0xpc3QiLCIxMjkiXV19)
    -   [New Foreign Direct Investment in the United States](https://apps.bea.gov/iTable/?reqid=2&step=3&isuri=1&step1prompt1=1&step2prompt3=1&step1prompt2=3#eyJhcHBpZCI6Miwic3RlcHMiOlsxLDIsMyw0LDUsNywxMF0sImRhdGEiOltbInN0ZXAxcHJvbXB0MSIsIjEiXSxbInN0ZXAycHJvbXB0MyIsIjEiXSxbInN0ZXAxcHJvbXB0MiIsIjMiXSxbIlN0ZXAzUHJvbXB0NCIsIjYyIl0sWyJTdGVwNFByb21wdDUiLCI5OSJdLFsiU3RlcDVQcm9tcHQ2IiwiMSwyIl0sWyJTdGVwN1Byb21wdDgiLFsiNjgiLCI2NiIsIjY1IiwiNjEiLCI2MCIsIjU4IiwiNTYiLCI1NSIsIjUyIiwiNDkiXV0sWyJTdGVwOFByb21wdDlBIixbIjI4OSJdXSxbIlN0ZXA4UHJvbXB0MTBBIixbIjk0Il1dXX0=)
    -   [Real GDP](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDNdLCJkYXRhIjpbWyJjYXRlZ29yaWVzIiwiU3VydmV5Il0sWyJOSVBBX1RhYmxlX0xpc3QiLCIxIl1dfQ==)
    -   [Personal Income and Its Disposition](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDNdLCJkYXRhIjpbWyJjYXRlZ29yaWVzIiwiU3VydmV5Il0sWyJOSVBBX1RhYmxlX0xpc3QiLCI1OCJdXX0=)

- Federal Reserve Bank
    -   [Selected Interests Rates](https://www.federalreserve.gov/releases/h15/)

- Bureau of Labor Statistics
    -   [CPI](https://fred.stlouisfed.org/series/CPILFESL)
    -   [Employment Cost Index](https://fred.stlouisfed.org/series/ECIALLCIV)
    -   [PPI](https://fred.stlouisfed.org/series/PPIFIS)
    -   [Housing Starts](https://fred.stlouisfed.org/series/HOUST)
    -   [Unemployment Rate](https://fred.stlouisfed.org/series/UNRATE)